原始数据：
id,姓名,爱好,住址
1,小明1,lol-book-movie,beijing:mashibing-shanghai:pudong
2,小明2,lol-book-movie,beijing:mashibing-shanghai:pudong
3,小明3,lol-book-movie,beijing:mashibing-shanghai:pudong
4,小明4,lol-book-movie,beijing:mashibing-shanghai:pudong
5,小明5,lol-movie,beijing:mashibing-shanghai:pudong
6,小明6,lol-book-movie,beijing:mashibing-shanghai:pudong
7,小明7,lol-book,beijing:mashibing-shanghai:pudong
8,小明8,lol-book,beijing:mashibing-shanghai:pudong
9,小明9,lol-book-movie,beijing:mashibing-shanghai:pudong

新建一个data文件在linux本地(hadoop-04)，并加入以上内容

create table psn
(
    id int,
    name string,
    likes array<string>,
    address map<string,string>
)
row format delimited
fields terminated by ','
collection items terminated by '-'
map keys terminated by ':';

https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL
https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML

load data local inpath '/root/data/data' into table psn; //加上local是从本地加载，不加local其实就是从hdfs的某一个目录往另一个目录加载数据
这一句加载数据加载得很快，原因是：Load operations are currently pure copy/move operations that move datafiles into locations corresponding to Hive tables
相当于hdfs dfs -put操作。而move指的是文件本身已经存在hdfs了。与put不同的是，每次load相同的本地文件上去不会报错，而是起一个别名（data_copy_2），
保存在HDFS中

再新建一个data2，并写入以下内容

11,小明11,lol-book-movie,beijing:mashibing-shanghai:pudong
21,小明21,lol-book-movie,beijing:mashibing-shanghai:pudong
31,小明31,lol-book-movie,beijing:mashibing-shanghai:pudong
41,小明41,lol-book-movie,beijing:mashibing-shanghai:pudong
51,小明51,lol-movie,beijing:mashibing-shanghai:pudong
61,小明61,lol-book-movie,beijing:mashibing-shanghai:pudong
71,小明71,lol-book,beijing:mashibing-shanghai:pudong
81,小明81,lol-book,beijing:mashibing-shanghai:pudong
91,小明91,lol-book-movie,beijing:mashibing-shanghai:pudong

然后执行：hdfs dfs -put data2 /user/hive_remote/warehouse/psn；
再从hive的命令行执行select * from psn; 则可以看到新数据也加进来了，所以，表所对应的目录下的所有的文件都会被处理，形成hive数据。实际上底层就是
HDFS的数据。Hive很傻，他只会读指定表格的目录下的文件，而不管它们是通过何种方式传上来的。MySQL在插入数据的时候类型和长度等必须匹配才能成功，
这叫"写时检查"。而hive是"读时检查"上传任意不规则的数据文件到hive表的目录之后，不规则数据在select之后会显示：
91	小明91	["lol","book","movie"]	{"beijing":"mashibing","shanghai":"pudong"}
NULL	NULL	NULL	NULL
NULL	NULL	NULL	NULL
不符合创建表的时候规定的格式就显示NULL，符合才正常显示数据

create table psn2
(
    id int,
    name string,
    likes array<string>,
    address map<string,string>
);

分隔符：^A是,^B是-,^C是: control+v再control+A输入，分隔符有^A ^B ^C... ^H一共是8个

对于,-:等分隔符，也有这么写的：'\001','\002','\003':
create table psn3
(
    id int,
    name string,
    likes array<string>,
    address map<string,string>
)
row format delimited
fields terminated by '\001'
collection items terminated by '\002'
map keys terminated by '\003';

//外部表.
// 内部表创建的时候存储在hive的默认目录中，外部表创建的时候需要写external关键字，同时需要指定location，指定存储目录
// 内部表在删除的时候将数据和元数据一起删除，外部表只删除元数据(MySQL)，但并不删除数据(HDFS)
// 应用场景：内部表：先创建表，在添加数据；外部表可以先创建表再添加数据，也可以先添加数据再创建表. 外部表确实用得比较多
// location可以用在导入S3表的时候：LOCATION 's3://volvo-region-emr-test/datainput/data/lyb'
create external table psn4
(
    id int,
    name string,
    likes array<string>,
    address map<string,string>
)
row format delimited
fields terminated by ','
collection items terminated by '-'
map keys terminated by ':'
location '/data1';

/*
讲道理，所有的select都应该是一个MR job，但是就并不是这样。
白天服务器跑实时性要求高的任务，晚上反之
*/

//分区表: 单分区
create table psn5
(
    id int,
    name string,
    likes array<string>,
    address map<string,string>
)
partitioned by (gender string)
row format delimited
fields terminated by ','
collection items terminated by '-'
map keys terminated by ':';

// 不能把partition写在（）里, hive把用来做分区的字段单拿出来的

//分区表: 多分区。多个分区插入的时候，各个分区要全写出来.
create table psn6
(
    id int,
    name string,
    likes array<string>,
    address map<string,string>
)
partitioned by (gender string,age int)
row format delimited
fields terminated by ','
collection items terminated by '-'
map keys terminated by ':';

//load data：
load data local inpath '/root/data/data' into table psn6 partition (gender='male',age=18);
load data local inpath '/root/data/data' into table psn6 partition (age=12,gender='male');
最后partition必须全部写出来，但是他们的顺序可以随便写，没关系，hive会按照每个分区以树状结构创建目录。
这样添加才可以：
alter table psn6 add partition(gender='girl',age=12);
alter table psn6 add partition(gender='girl',age=13);
但是删除的时候可以只指定一个分区，会把当前表的所有满足条件的分区（hdfs中的子目录）都会删除：
alter table psn6 drop partition(age=12);
当数据进入hive的时候，需要根据数据的某一个字段向hive表中插入数据，此时无法满足需求，因此需要使用动态分区。
分区粒度太大或太小都不好，太小了遍历目录的层数太多，要根据业务来制定.

// 直接把数据在对应的目录和分区子目录里放置好，执行下面的建external表语句的时候，可以创建，但是读不到数据，是因为元数据在MySQL中没有
create external table psn7
(
    id int,
    name string,
    likes array<string>,
    address map<string,string>
)
partitioned by (age int)
row format delimited
fields terminated by ','
collection items terminated by '-'
map keys terminated by ':'
location '/lisz';
这时候可以修复分区:
msck repair table psn7;

create table psn8
(
    id int,
    name string,
    likes array<string>,
    address map<string,string>
)
partitioned by (age int)
row format delimited
fields terminated by ','
collection items terminated by '-'
map keys terminated by ':'
location '/lisz';

create table psn9
(
    id int,
    name string
)
row format delimited
fields terminated by ','
collection items terminated by '-'
map keys terminated by ':';

create table psn10
(
    id int
);

from psn
insert overwrite table psn9
select id,name
insert into table psn10
select id;
//会比较慢，跑MR任务了. 实际工作中用的很多，存储中间临时表
// hive里的数据不能修改，其实也可以，但是配置非常麻烦，要支持事务才行，需要很多配置，因为底层的HDFS不支持修改

//把查询结果存放在某个路径下的文件里（一般不用）
insert overwrite local directory '/root/data/tmp'
row format delimited
fields terminated by ','
collection items terminated by '-'
map keys terminated by ':'
select * from psn;

CREATE TABLE logtbl (
host STRING,
identity STRING,
t_user STRING,
time STRING,
request STRING,
referer STRING,
agent STRING)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'
WITH SERDEPROPERTIES (
"input.regex" = "([^ ]*) ([^ ]*) ([^ ]*) \\[(.*)\\] \"(.*)\" (-|[0-9]*) (-|[0-9]*)"
) STORED AS TEXTFILE;
括号里面的是要保留的，其余的[]""属于要去除的

启动hiveserver2： hive --service hiveserver2
之后才能访问 http://hadoop-03:10002 hiveserver2可以用beeline打开连接
执行beeline就可以登录beeline客户端。 用：
!connect jdbc:hive2://hadoop-03:10000/default root ^abc123$
beeline的命令执行的时候要加!
!close 退出当前hiveserver2
!quite 退出整个beeline
beeline -u jdbc:hive2://hadoop-03:10000/default 直接登录
beeline -u jdbc:hive2://hadoop-03:10000/default -n root 123 也行，用户名密码在hiveserver2这里无效
hiveserver2可以支持web图形化界面的, 而且hiveserver2也不能做增删改，只能查询，是给查询用户使用的。
Beeline可以不用再hive集群里执行，连10000端口就行
启动hiveserver2的时候其实也启动了一个类似metastore的服务

hive不能impersonate错误如果爆出来的话，要在两台nn中的core-site.xml文件里加上：
<property>
    <name>hadoop.proxyuser.root.groups</name>
    <value>*</value>
</property>
<property>
    <name>hadoop.proxyuser.root.hosts</name>
    <value>*</value>
</property>
并在两台nn上执行：
hdfs dfsadmin -fs hdfs://hadoop-01:8020 -refreshSuperUserGroupsConfiguration
hdfs dfsadmin -fs hdfs://hadoop-02:8020 -refreshSuperUserGroupsConfiguration
来使配置生效且无需重启hadoop集群
取array中第一项的value
select likes[1] from psn;
已知key，取map中的value
select address['beijing'] from psn;

hive实现wordcount：
先把文本文件放入hdfs的/wc中，然后再hive执行：create external table wc(line string) location '/wc';  创建表
再利用函数和子查询往结果表中插入数据：
from (select explode(split(line, ' ')) word from wc) t insert into wc_result select word, count(word) group by word;
最后查看结果：
hive> select * from wc_result;
OK
wc_result.word	wc_result.ct
hadoop	3
hello	2
hi	3
hive	2
spark	1

查看map、reduce和总耗时定位到底是哪个阶段比较慢，除了map和reduce之外的时间是被用来申请服务器资源了
什么操作会触发MR？hive有些会做优化，不会触发MR。
sqoop是做关系型数据库跟大数据平台数据迁移的


create table psn21
(
id int,
name string,
age int,
gender string,
likes array<string>,
address map<string,string>
)
row format delimited
fields terminated by ','
collection items terminated by '-'
map keys terminated by ':';

create table psn22
(
id int,
name string,
likes array<string>,
address map<string,string>
)
partitioned by (age int,gender string)
row format delimited
fields terminated by ','
collection items terminated by '-'
map keys terminated by ':';

insert into table psn22
partition(age,gender)
select id, name, likes, address, age, gender from psn21;

create table psn31
(
id int,
name string,
age int
)
row format delimited
fields terminated by ','
;

create table psnbucket
(
id int,
name string,
age int
)
clustered by (age) into 4 buckets
row format delimited
fields terminated by ','
;

insert into table psnbucket select * from psn31;

select count(distinct(likes_count)), count(distinct(address_city)) from psn
lateral view explode(likes) psn as likes_count
lateral view explode(address) psn as address_city, address_district;

select id, myCol1 from psn
lateral view explode(likes) psn as myCol1;

explode(likes) 这样的在一个sql里面只能有一个

索引存的是某一列在硬盘上的位置，包括在哪个目录的哪个文件，及其偏移量

一般是在只读的情况下，表的数据都生成完了之后创建索引。

数据量比较小的时候，加了索引之后查询反而变慢，这是因为会先查索引表，躲得开一次文件。而MySQL的索引存在内存里，以B+树的形式存储


Join

join一般是指inner join，两张表里必须有完全匹配的记录，left join是指：把左表记录显示全了，而不管右表，右表如果有则正常显示，如果没有，显示NULL；right join反之。FULL：都显示，匹配不上则显示
NULL

left semi join代替的是 in 或者 exists ，select中不能包括右表的字段

多次连续join的时候，尽量多写相同的连接字段在各个on () 里面


用MR做join的时候 on(a.id = b.id)，两个map任务，相同的id为key，在value里面标识一下来自于那个文件。reduce的时候，相同的key放在同一台reducer. 在reducer判断一下是不是都来自a文件
如果是，则没必要做join了，如果不是，则join。MR分为split、map、shuffle、reduce 4 个阶段，其中shuffle最耗时，效率的瓶颈在IO操作。mapjoin对于一张大表和一张小表有效，把小表放入内存做join

Hive一般不做分页，limit的意思是限制输出，并不是分页


<property>
                <name>hadoop.proxyuser.root.groups</name>
                <value>*</value>
        </property>
        <property>
                <name>hadoop.proxyuser.root.hosts</name>
                <value>*</value>
        </property>

是让root用户代理其他用户，使之有权限修改文件，并且文件的属主是这个其他用户，而不是root 会自行判断能不能并行计算，然后就可以在适当的情况下并行计算了

create table person1
(
    id int,
    name string
)
row format delimited
fields terminated by ',';

create external table person (
    id int,
    name string
    )
    stored by 'org.elasticsearch.hadoop.hive.EsStorageHandler'
    TBLPROPERTIES('es.resource' = 'person',
    'es.index.auto.create' = 'false',
    'es.nodes' = '192.168.1.3',
    'es.port'='9200'
);


 create table psn21
    (id int, name string, age int, gender string, likes array<string>, address map<string, string>)
    row format delimited
    fields terminated by ','
    collection items terminated by '-'
    map keys terminated by ':';

create table psn22
    (id int, name string, likes array<string>, address map<string, string>)
    partitioned by (age int, gender string)
    row format delimited
    fields terminated by ','
    collection items terminated by '-'
    map keys terminated by ':';

create table psn31 (id int, name string, age int)
    row format delimited fields terminated by ',';
create table psnbucket (id int, name string, age int)
    clustered by (age) into 4 buckets
    row format delimited fields terminated by ',';

create index t1_index on table psn(name)
as 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler' with deferred rebuild in table t1_index_table;

create index t2_index on table psn(name)
as 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler' with deferred rebuild;